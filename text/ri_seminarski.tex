% !TEX encoding = UTF-8 Unicode
\documentclass[a4paper]{article}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={150mm,240mm},
 left=30mm,
 top=30mm,
 }

\usepackage{color}
\usepackage{url}
\usepackage[T2A]{fontenc} % enable Cyrillic fonts
\usepackage[utf8]{inputenc} % make weird characters work
\usepackage{graphicx}
\usepackage[]{algorithm2e}
\usepackage{caption}
\usepackage{float}

\usepackage[english,serbian]{babel}
%\usepackage[english,serbianc]{babel} %ukljuciti babel sa ovim opcijama, umesto gornjim, ukoliko se koristi cirilica

\usepackage[unicode]{hyperref}
\hypersetup{colorlinks,citecolor=green,filecolor=green,linkcolor=blue,urlcolor=blue}

\usepackage{listings}

%\newtheorem{primer}{ĞŸÑ€Ğ¸Ğ¼ĞµÑ€}[section] %Ä‡iriliÄni primer
\newtheorem{primer}{Primer}[section]

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ 
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\scriptsize\ttfamily,        % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  firstnumber=1000,                % start line enumeration with line 1000
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=Python,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=2,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
  title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\begin{document}

\title{PodeÅ¡avanje teÅ¾ina neuronske mreÅ¾e upotrebom optimizacionog algoritma\\ \small{Seminarski rad u okviru kursa\\RaÄunarska inteligencija\\ MatematiÄki fakultet}}

\author{Nikola StameniÄ‡, Lea PetkoviÄ‡\\ mi16177@alas.matf.bg.ac.rs, mi16163@alas.matf.bg.ac.rs}

\maketitle

\abstract{
Ovaj rad se bavi podeÅ¡avanjem teÅ¾ina neuronske mreÅ¾e uz pomoÄ‡ algoritma optimizacije rojem Äestica.
Pored osnovnog teorijskog pristupa datim pojmovima, ANN i PSO, rad sadrÅ¾i i praktiÄnu primenu ovakvog
pristupa podeÅ¡avanja teÅ¾ina. Kako bi imali oseÄ‡aj koliko su rezultati dobri, autori ovog rada svoje rezultate
uporeÄ‘uju sa rezultatima autora rada \emph{Designing Artificial Neural Networks Using Particle Swarm Optimization Algorithms} \cite{hindawi}.
\\
\\
\textbf{KljuÄne reÄi:} PSO, neuronske mreÅ¾e, ANN, optimizacija rojem Äestica, Keras, Iris, Wine, Breast Cancer.
}
\tableofcontents

\newpage

\section{Uvod}
\label{sec:uvod}

PodeÅ¡avanje teÅ¾ina neuronske mreÅ¾e (eng. \emph{Artificial Neural Network, ANN}) za cilj ima nalaÅ¾enje optimalnog skupa teÅ¾ina, kako bi Äitava neuronska mreÅ¾a za odreÄ‘ene podatke davala Å¡to
taÄnije izlaze, kako u primeni za klasifikaciju, tako i u primeni za regresiju. Neuronska mreÅ¾a moÅ¾e biti koriÅ¡Ä‡ena na jako
bitnim istraÅ¾ivanjima, tako da podeÅ¡avanje teÅ¾ina moÅ¾e igrati jako bitnu ulogu. Jedan od naÄina na koje moÅ¾emo podesiti teÅ¾ine jeste algoritmom optimizacije rojem Äestica, skraÄ‡eno (eng.
\emph{Particle Swarm Optimization}) \cite{PSOANN}. PSO je algoritam nastao inspirisan prirodom, odnosno kretanjem jata ptica u potrazi za hranom, pa moÅ¾e predstavljati dobar algoritam za
podeÅ¡avanje teÅ¾ina neuronske mreÅ¾e. Problem sa samim algoritmom moÅ¾e biti to Å¡to postoji moguÄ‡nost zaglavljivanja u lokalnom optimumu, Å¡to u startu ne garantuje nalaÅ¾enja najboljeg moguÄ‡eg
reÅ¡enja. Problemom podeÅ¡avanja teÅ¾ina neuronske mreÅ¾e, pomoÄ‡u optimizacije rojem Äestica, bavili su se mnogi istraÅ¾ivaÄi \cite{hindawi}.


\section{Neuronske mreÅ¾e}
\label{neuronskemreze}

Neuronska mreÅ¾a je sistem koji vrÅ¡i mapiranje izmeÄ‘u ulaza i izlaza problema.
Neuronske mreÅ¾e zapravo predstavljaju parametrizovanu reprezentaciju koja se moÅ¾e koristiti za aproksimaciju raznih funkcija \cite{hindawi}.
MatematiÄkom optimizacijom nekog od kriterijuma kvaliteta vrÅ¡i se pronalaÅ¾enje odgovarajuÄ‡ih parametara. 

Neuronske mreÅ¾e uÄe informacije kroz proces treniranja u nekoliko iteracija. Kada je proces uÄenja zavrÅ¡en, 
neuronska mreÅ¾a je spremna i sposobna da klasifikuje nove informacije, predvidi ponaÅ¡anje, ili aproksimira nelinearnu funkciju problema. 
Njena struktura sastoji se od skupa neurona, predstavljenih
funkcijama, koji su meÄ‘usobno povezani sa ostalim neuronima organizovanim u slojeve. 

Struktura neuronske mreÅ¾e se razlikuje po broju slojeva. Prvi sloj jeste ulazni sloj, poslednji sloj jeste izlazni, a svi slojevi izmeÄ‘u se nazivaju skrivenim
slojevima. Slojevi su meÄ‘usobno potpuno povezani. Slojevi komuniciraju zahvaljujuÄ‡i tome Å¡to je izlaz svakog neurona, iz prethodnog sloja, povezan sa
ulazima svih neurona iz narednog sloja. JaÄina veza kojom su neuroni meÄ‘usobno povezani se naziva teÅ¾inski faktor (eng. \emph{weight}). NajÄeÅ¡Ä‡e ima
3 sloja.

Postoje razliÄite vrste neuronskih mreÅ¾a. Mozemo ih klasifikovati prema: broju slojeva (jednoslojne i viÅ¡eslojne), vrsti veza izmeÄ‘u neurona, smeru
prostiranja informacija (neuronske mreÅ¾e sa propagacijom unapred ili unazad) \cite{website}, vrsti podataka itd. 

Njihove primene su mnogobrojne, obzirom da predstavljaju najÄeÅ¡Ä‡e primenjivanu metodu maÅ¡inskog uÄenja. Neke od primena su: kategorizacija teksta,
medicinska dijagnostika, prepoznavanje objekata na slikama, autonomna voÅ¾nja, igranje igara poput igara na tabli ili video igara, maÅ¡insko prevoÄ‘enje
prirodnih jezika, prepoznavanje rukom pisanih tekstova itd. 

\subsection{Keras Pajton biblioteka}
\label{subsec:keras}

Keras je biblioteka za neuronske mreÅ¾e, napisana u Pajtonu. Keras radi na platformi za maÅ¡insko uÄenje  \textit{TensorFlow}.
Razvijena je sa ciljem da omoguÄ‡i brzo eksperimentisanje sa neuronskim mreÅ¾ama \cite{keraswebsite},
da bude razumljiva, modularna i proÅ¡iriva.

Pored, veÄ‡ pomenute, \textit{TensorFlow} platforme, ova biblioteka radi i na: \textit{Microsoft Cognitive Toolkit, R, Theano, ili PlaidML} platformama. 
Nastala je kao deo istraÅ¾ivanja u okviru projekta ONEIROS (eng. \emph{Open-ended Neuro-Electronic Intelligent Robot Operating System}), 
a njen autor i odrÅ¾avaoc je gugl inÅ¾enjer -  FranÃ§ois Chollet.
% \begin{verbatim}
% Å¡ÄÄ‡Å¾Ä‘
% \end{verbatim}

\section{Optimizacija rojem Äestica}
\label{subsec:pso}
U ovoj sekcije biÄ‡e objaÅ¡njen sam algoritam optimizacije rojem Äestica. NajviÅ¡e vremena biÄ‡e posveÄ‡eno originalnom algoritmu PSO-a kao i drugoj generaciji algoritma PSO (eng. \textit{The Secong Generation of PSO}), kao i novom modelu algoritma PSO 
(eng. \textit{A New Model of PSO}).


\subsection{Originalni algoritam PSO}
\label{subsec:opso}
Algoritam PSO je metod za optimizaciju neprekidne nelinearne funkcije, koji je predloÅ¾io Eberhart.
Sam algoritam je inspirisan posmatranjem socijalnog i kolektivnog ponaÅ¡anja u kretanju jata ptica pri potrazi za hranom ili preÅ¾ivaljavanjem.
PSO je nadahnut kretanjem najboljeg Älana populacije i njegovog iskustva. Metafora govori da se skup reÅ¡enja kreÄ‡e prostorom pretrage 
sa ciljem da naÄ‘e Å¡to bolju poziciju, reÅ¡enje \cite{hindawi}.
\\
Populacijom se smatra grupa Äestica \textit{i} gde svaka predstavlja poziciju \textbf{\textbf{$x_i \in R^D$, i = 1,...,M}} u viÅ¡edimenzionom prostoru.
ÄŒestice se evaluiraju u posebnoj funkciji optimizacije, kako bi se odredila njihova prilagoÄ‘enost i saÄuvala najbolja vrednost. Svaka Äestica se kreÄ‡e po
prostoru pretrage u zavisnosti od funkcije brzine \textbf{$v_i$} koja u obzir uzima globalno najbolju poziciju u populaciji ($p_g \in R^D$ - socijalna
komponenta) kao i najbolju poziciju date Äestice ($p_i \in R^D$ - kognitivna komponenta). ÄŒestice Ä‡e se kretati u svakoj iteraciji na drugu poziciju,
dok ne dostignu optimalnu poziciju. U svakom momentu \textit{t}, brzina Äestice \textit{i} se aÅ¾urira koristeÄ‡i: 
\begin{center}
\textbf\textit{$v_i(t+1) = \omega v_i(t) + c_1 r_1(p_i (t) - x_i (t)) + c_2 r_2 (p_g (t) - x_i (t))$}
\end{center}
gde je $\omega$ inertna teÅ¾ina i obiÄno je postavljena da varira linearno od 1 do 0 tokom iteracije, $c_1$ i $c_2$ su koeficijenti ubrzanja, $r_1$ i $r_2$
su sluÄajni brojevi iz uniformne (0,1) raspodele. Ubrzanje \textbf{$v_i$} je ograniÄeno izmeÄ‘u [$v_{min}, v_{max}$]. AÅ¾uriranjem ubrzanja na ovaj 
naÄin dozvoljavamo Äestici $i$ da traÅ¾i najbolju poziciju \textbf{$p_i(t)$}, dok se najbolje globalno reÅ¡enje raÄuna \cite{hindawi}:
\begin{center}
\textbf\textit{$x_i(t+1) = x_i(t) + v_i(t+1)$}
\end{center} 
% Optimizacija rojem Äestica se moÅ¾e prikazati sledeÄ‡im pseudokodom:

%Za sva reÅ¡enja (Äestice)  ğ‘–  iz skupa reÅ¡enja (roja)  ğ‘‹ :

%Izabrati koordinate vektora  ğ‘¥ğ‘–  uniformno iz intervala  (ğ‘™,ğ‘¢) 
%ğ‘ğ‘–=ğ‘¥ğ‘– 
%Ako je  ğ‘“(ğ‘¥ğ‘–)<ğ‘“(ğ‘”)  tada  ğ‘”=ğ‘¥ğ‘– 
%OznaÄiti koordinate vektora  ğ‘£ğ‘–  da su jednaki nuli ili uniformno iz %intervala  (âˆ’|ğ‘¢âˆ’ğ‘™|,|ğ‘¢âˆ’ğ‘™|) 
%Dok nije ispunjen kriterijum zaustavljanja:

%Za sve Äestice  ğ‘–  iz roja  ğ‘‹ :
%Izabrati brojeve  ğ‘Ÿğ‘”  i  ğ‘Ÿğ‘  uniformno iz intervala  (0,1) 
%ğ‘£ğ‘–=ğ‘ğ‘£ğ‘£ğ‘–+ğ‘ğ‘ğ‘Ÿğ‘(ğ‘ğ‘–âˆ’ğ‘¥ğ‘–)+ğ‘ğ‘”ğ‘Ÿğ‘”(ğ‘”âˆ’ğ‘¥ğ‘–) 
%ğ‘¥ğ‘–=ğ‘¥ğ‘–+ğ‘£ğ‘– 
%Ako je  ğ‘“(ğ‘¥ğ‘–)<ğ‘“(ğ‘ğ‘–)  tada  ğ‘ğ‘–=ğ‘¥ğ‘– 
%Ako je  ğ‘“(ğ‘¥ğ‘–)<ğ‘“(ğ‘”)  tada  ğ‘”=ğ‘¥ğ‘– 
%ReÅ¡enje je vektor  ğ‘” , a vrednost reÅ¡enja broj  ğ‘“(ğ‘”)


\subsection{Druga generacija PSO algoritma}
\label{subsec:sgpso}

Druga generacija PSO algoritma je unapreÄ‘enje originalnog PSO algoritma, koja u obzir uzima tri aspekta: lokalni optimum svih Äestica, 
globalno najbolje reÅ¡enje, i novi koncept - geometrijski centar optimalne populacije. Autor knjige \textit{"{}Second Generation Particle Swarm Optimization"} 
objaÅ¡njava da ptice odrÅ¾avaju  odreÄ‘enu distancu izmeÄ‘u centra jata (hrane). Jata ptica uvek ostaju u istom regionu neko vreme, 
tokom kojeg Ä‡e centar jata ostati nepomeren u oÄima Äestica. Nakon toga, jato se kreÄ‡e na sledeÄ‡i region, tada sve Äestice moraju 
odrÅ¾ati odreÄ‘enu distancu sa centrom jata.

\subsection{Novi model PSO-a}
\label{subsec:nmpso}

Ovaj algoritam je predloÅ¾io Garo (eng. \textit{Garro}), bazirao ga je na osnovu ideja drugih autora koji su predlagali unapreÄ‘enje originalnog 
PSO algoritma. Shi i Eberhart su predlagali linearno variranje inertnih teÅ¾ina kroz generacije, Å¡to je znatno unapredilo performanse originalnog PSO algoritma.
Yu je razvio strategiju da kada se kroz generacije globalno najbolje reÅ¡enje ne poboljÅ¡ava, svaka Äestica \textit{$i$} biva izabrana sa 
predefinisanom verovatnoÄ‡om, a zatim je dodat sluÄajni Å¡um svakom vektoru brzine dimenzije $v_i$ izabrane Äestice \textit{$i$}. 
Bazirano na nekim evolutivnim shemama GenetiÄkih algoritama, nekoliko efektnih mutacija i ukrÅ¡tanja su predloÅ¾ene za PSO.

\section{PodeÅ¡avanje teÅ¾ina neuronske mreÅ¾e pomoÄ‡u optimizacije rojem Äestica}
\label{sec:podesavanjetezina}

Treniranje neuronske mreÅ¾e, odnosno podeÅ¡avanje teÅ¾ina, izvrÅ¡eno je pomoÄ‡u optimizacije rojem Äestica. 
Napravljena je troslojna neuronska mreÅ¾a (ulazni, skriveni i izlazni sloj) pomoÄ‡u Keras biblioteke u Pajtonu, pomoÄ‡u koje se reÅ¡avaju klasifikacioni problemi. 
Dve treÄ‡ine podataka koriÅ¡Ä‡ene su za trening podatke, a ostatak Äini test podatke. Skriveni sloj sadrÅ¾i 100 jedinica/Ävorova, dok je broj izlaza 
ekvivalentan broju razliÄitih klasa u odgovarajuÄ‡em skupu podataka. Pri kompiliranju Keras modela koriÅ¡Ä‡en je \textit{adam} optimizator,
ali on nema nikakvog uticaja, veÄ‡ je naveden iz sintaksnih razloga. Kao metrika koriÅ¡Ä‡ena je preciznost (eng. \emph{accurancy}). 

Kao Å¡to je ranije napomenuto, ANN trenirana je pomoÄ‡u PSO algoritma. PSO koristi prvo 30, Äestica i 300 iteracija, a zatim 50 Äestica i 1000 iteracija. 
Å alju se trening podaci i neuronska mreÅ¾a. TeÅ¾ine neuronske mreÅ¾e predstavljaju pozicije Äestica. Inicijalne teÅ¾ine biraju se nasumiÄno unutar intervala [-2, 2],
a aÅ¾uriranje teÅ¾ina se vrÅ¡i prema formuli: 

\begin{center}
{$x_i(t+1) = x_i(t) + v_i(t+1)$}
\end{center}
objaÅ¡njenoj u poglavlju \ref{subsec:opso}. Parametri $c_1$, $c_2$ i $w$ izabrani su u odnosu na vrednosti istih tih parametara u radu \cite{hindawi},
odnosno $w$ je 0.3, dok su $c_1$ i $c_2$ za 0.5 manji, taÄnije 0.5 i 1.0 redom, jer tako algoritam daje bolje rezultate. 

Kao funkcija cilja koriÅ¡Ä‡ena je preciznost. TakoÄ‘e, u cilju izbegavanja zaglavljivanja u lokalnom minimumu algoritam se pokreÄ‡e 
nekoliko puta i pozicije najgorih pet Äestica se nasumiÄno menjaju. Najbolja Äestica je upravo ona sa najveÄ‡om preciznoÅ¡Ä‡u.

\subsection{Rezultati}
\label{rezultati}

U ovoj sekciji biÄ‡e predstavljeni eksperimentalni rezultati dobijeni nad skupovima: \textit{Iris, Breast Cancer i Wine}, kao i uporeÄ‘eni sa
rezultatima drugih istraÅ¾ivaÄa koji su se bavili ovom temom i koristili navedene skupove \cite{hindawi}. Stopa prepoznavanja neuronske
mreÅ¾e procenjena je i raÄunata formulom:

\begin{center}
{$wrr = 0.4*(Tr_r r) + 0.6*(Te_r r)$}, 
\end{center}
gde je $Tr_r r$ preciznost nad trening podacima, a $Te_r r$ preciznost dobijenu nad test podacima. KoriÅ¡Ä‡eni su faktori 0.4 i 0.6 
kako bi se izbegla visoka vrednost $wrr$ (eng. \emph{weighted recognition rate}) usled visoke preciznosti nad trening podacima 
(kada to nije sluÄaj i sa test podacima). U nastavku sledi viÅ¡e o rezultatima na svakom od skupova pojedinaÄno.

\subsubsection{Skup Iris}
\label{iris}

Ovaj skup podataka nalazi se u Pajton paketu za maÅ¡insko uÄenje: Scikit-learn. Sastoji se od 3 vrste cveta Iris - Setosa, Versicolour, Virginica.
SadrÅ¾i 150 instanci i ima 5 atributa koji predstavljaju duÅ¾inu i Å¡irinu latica (eng. \emph{petal length, petal width}), duÅ¾inu ÄaÅ¡iÄnog listiÄ‡a 
(eng. \emph{sepal length, sepal width}) i vrstu kojoj cvet pripada (eng. \emph{species}). 

Procenjena stopa prepoznavanja (wrr) neuronske mreÅ¾e, koja je dobijena na skupu Iris, najveÄ‡a je od svih testiranih i ona iznosi vise od 95\%, 
kako na trening, tako i na test podacima. Na slici \ref{fig:irisslika} moÅ¾e se videti i zakljuÄiti da je dobijena wrr autora ovog rada i preciznost 
autora ranije pomenutog rada \cite{hindawi} pribliÅ¾no ista.

\begin{figure}[h!]
\centering
\captionsetup{justification=centering,margin=2cm}
\begin{center}
\includegraphics[scale=0.4]{img/iriswrr.png}
\end{center}
\caption{Preciznost na podacima skupa Iris. Sa leve strane nalaze se rezultati autora, a desno rezultati iz literature \cite{hindawi} }
\label{fig:irisslika}
\end{figure}

\subsubsection{Skup Breast cancer}
\label{breast}

Skup podataka Breast cancer se, takoÄ‘e, nalazi u Pajton paketu za maÅ¡insko uÄenje: Scikit-learn. Predstavlja jednostavan skup 
podataka koji se koristi za binarnu klasifikaciju. ViÅ¡e informacija o skupu dato je u tabeli \ref{table_bc}.

\begin{table}[h!]
\begin{center}
\caption{Informacije o skupu Breast Cancer}
\begin{tabular}{|p{4cm}|p{2cm}|}
\hline
Klase             & 2              \\ \hline
Instance po klasi & 212(M), 357(B) \\ \hline
Ukupno instanci   & 569            \\ \hline
Dimenzija         & 30             \\ \hline
Atributi          & real, positive \\ \hline
\end{tabular}\par
\label{table_bc}
\bigskip
\end{center} 
\end{table}

Rezultati dobijeni na neuronskoj mreÅ¾i za skup Breast Cancer neznatno su slabiji od rezultata nad skupom podataka Iris.
Dobijena wrr iznosi oko 90-92\%. Na slici \ref{fig:breastslika} moÅ¾e se videti poreÄ‘enje rezultata. PrimeÄ‡uje se da su 
rezultati autora \cite{hindawi} neÅ¡to bolji, ali i dalje su obe preciznosti zadovoljavajuÄ‡e. 

\begin{figure}[H]
\centering
\captionsetup{justification=centering,margin=2cm}
\begin{center}
\includegraphics[scale=0.4]{img/bcwrr.png}
\end{center}
\caption{Preciznost na podacima skupa Breast Cancer. Sa leve strane nalaze se rezultati autora, a desno rezultati iz literature \cite{hindawi} }
\label{fig:breastslika}
\end{figure}

\subsubsection{Skup Wine}
\label{wine}

Wine skup podataka predstavlja klasiÄan, jednostavan i viÅ¡eklasni klasifikacioni skup podataka u paketu za Scikit-learn. ViÅ¡e informacija o skupu dato je u tabeli \ref{table_wine}.

\begin{table}[h!]
\begin{center}
\caption{Informacije o skupu Wine}
\begin{tabular}{|p{4cm}|p{2cm}|}
\hline
Klase             & 3              \\ \hline
Instance po klasi & 59,71,48       \\ \hline
Ukupno instanci   & 178            \\ \hline
Dimenzija         & 13             \\ \hline
Atributi          & real, positive \\ \hline
\end{tabular}\par
\label{table_wine}
\bigskip
\end{center} 
\end{table}

Najslabiji rezultati dobijeni su upravo na ovom skupu, kako u literaturi, tako i u ovom radu. U radu \cite{hindawi} dobijena je 
wrr oko 85\%, dok su autori ovog rada dobili slabije rezultate - oko 70\%, Å¡to se moÅ¾e videti na priloÅ¾enom grafiku \ref{fig:wineslika}. 

\begin{figure}[h!]
\centering
\captionsetup{justification=centering,margin=2cm}
\begin{center}
\includegraphics[scale=0.4]{img/winewrr.png}
\end{center}
\caption{Preciznost na podacima skupa Wine. Sa leve strane nalaze se rezultati autora, a desno rezultati iz literature \cite{hindawi} }
\label{fig:wineslika}
\end{figure}


\section{ZakljuÄak}
\label{sec:zakljucak}

IstraÅ¾ivanjem podeÅ¡avanja teÅ¾ina neuronske mreÅ¾e pomoÄ‡u optimizacije rojem Äestica, zakljuÄeno je da rezultati donekle zavise od skupa, 
Å¡to bi moglo sugerisati da je potrebno posvetiti viÅ¡e paÅ¾nje pretprocesiranju onih skupova na kojima su rezultati slabiji ili promenu 
odreÄ‘enih parametara (npr. poveÄ‡an broj iteracija pri uÄenju), u cilju veÄ‡e preciznosti kasnije.

Dalje istraÅ¾ivanje moglo bi da se fokusira na koriÅ¡Ä‡enju drugih optimizacionih algoritama za podeÅ¡avanje teÅ¾ina neuronskih mreÅ¾a, 
kao Å¡to su: optimizacija mravljom kolonijom (eng. \emph{Ant Colony Optimization, ACO}), optimizacija kolonijom 
pÄela (eng. \emph{Bee Colony Optimization, BCO}), simulirano kaljenje (eng. \emph{Simulated Annealing, SA}) itd. Drugi pravac 
istraÅ¾ivanja mogao bi se baviti unapreÄ‘enjem samog PSO algoritma i njegovih parametara, radi dobijanja boljih rezultata.

\addcontentsline{toc}{section}{Literatura}
\appendix
\bibliography{seminarski} 
\bibliographystyle{plain}

\newpage
\appendix
\section{Prikaz rezultata}

\begin{figure}[h!]
\centering
\captionsetup{justification=centering,margin=2cm}
\begin{center}
\includegraphics[scale=0.4]{img/train.png}
\end{center}
\caption{Uporedni prikaz rezultata dobijenih na trening podacima, za sva tri skupa}
\label{fig:train_appendix}
\end{figure}


\begin{figure}[h!]
\centering
\captionsetup{justification=centering,margin=2cm}
\begin{center}
\includegraphics[scale=0.4]{img/test.png}
\end{center}
\caption{Uporedni prikaz rezultata dobijenih na test podacima, za sva tri skupa}
\label{fig:test_appendix}
\end{figure}

\section{Kod}

\begin{lstlisting}
import pandas as pd               # data processing, CSV file I/O (e.g. pd.read_csv)
import numpy as np                # linear algebra
from numpy.random import seed
from numpy.random import rand
from sklearn import datasets 
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from keras.models import Sequential
from keras.layers import Dense
from keras import losses, optimizers
from keras.utils import to_categorical

from matplotlib import pyplot as plt

\end{lstlisting}


\begin{lstlisting}
class Particle:
    
    def __init__(self, weights, velocity, omega, c1, c2, 
                 lower_bound, upper_bound, particle_id):
        self.id = particle_id
        self.weights = weights
        self.best_weights = weights
        self.velocity = np.array(velocity)
        self.omega = omega
        self.c1 = c1
        self.c2 = c2
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self.best_valuation = 0
        self.history = []
        
    def update(self, best_particle):
        self.update_velocity(best_particle)
        self.check_velocity()
        self.update_particle()
        
        return self.weights
    
    def update_velocity(self, bp):
        # v = Wv + c1r1(pi - xi) + c2r2(g - xi)
        # v        - ubrzanje 
        # W        - omega 
        # c1 i c2  - unapred zadati parametri
        # r1 i r2  - slucajni brojevi iz (0,1) uniformne raspodele
        # pi       - najbolje resenje trenutne jedinke
        # g        - najbojle globalno resenje
        r1 = np.random.random()
        r2 = np.random.random()
        
        self.velocity = self.omega * self.velocity + self.c1*r1*(np.add(np.array(self.best_weights), (-1)*np.array(self.weights))) + self.c2*r2*(np.add(np.array(bp), (-1)*np.array(self.weights))) 
        
    def check_velocity(self):
        for i in range(len(self.velocity)):
            if(i%2 == 1):
                for j in range(len(self.velocity[i])): 
                    if(self.velocity[i][j] < self.lower_bound):
                        self.velocity[i][j] = self.lower_bound
                    
                    if(self.velocity[i][j] > self.upper_bound): 
                        self.velocity[i][j] = self.upper_bound
            else: 
                for j in range(len(self.velocity[i])):
                    for k in range(len(self.velocity[i][j])):
            
                        if(self.velocity[i][j][k] < self.lower_bound):
                            self.velocity[i][j][k] = self.lower_bound
                
                        if(self.velocity[i][j][k] > self.upper_bound): 
                            self.velocity[i][j][k] = self.upper_bound
        
        return 
    
    
    def update_particle(self):
        # xi = xi + vi
        self.weights = np.add(np.array(self.weights), self.velocity)
        
    def update_valuation(self, valuation):
        self.history.append(valuation)
        
        if(valuation > self.best_valuation):
            self.best_valuation = valuation
            self.best_weights = self.weights
            
    def get_weights(self):
        return self.weights
    
    def set_weights(self, new_weights):
        self.weights = new_weights
    

\end{lstlisting}


\begin{lstlisting}

class PSO:
    def __init__(self, num_particles, num_iters, training_x, training_y, model, shapes, c1, c2, w, lower_bound, upper_bound):
        self.num_iters = num_iters
        self.training_x = training_x
        self.training_y = training_y
        self.model = model
        self.best_particle = self.make_velocity(shapes)
        self.best_evaluation = 0
        self.history = []
        self.shapes = shapes
        self.lower_bound = lower_bound
        self.upper_bound = upper_bound
        self.particles = self.make_particles(num_particles, shapes, c1, c2, w, lower_bound, upper_bound)
        self.evaluate_particles()
        self.combo = 0
        self.w = w
        self.c1 = c1
        self.c2 = c2
        
        
    def make_particles(self, size, shape, c1, c2, w, low, upp):
        particles = []
        
        velocity = self.make_velocity(shape)
        
        for i in range(size):
            weights = self.make_weights(shape, low, upp)
            particles.append(Particle(weights, velocity, w, c1, c2, low, upp, i))
            
        return particles
    
    def make_velocity(self, shapes):
        velocity = []
        
        for shape in shapes:
            velocity.append(np.zeros(shape))
        
        return velocity
                
        
    def make_weights(self, shapes, low, upp):
        weights = []
        
        for shape in shapes:
            if(len(shape) == 1):
                # wght da dobijemo vrednosti iz (0, upp-low)
                wght = rand(shape[0])*(upp-low)
                rec = np.full(shape, low)
                
                weights.append(np.add(wght, rec))
                
            else:
                wght = rand(shape[0], shape[1])*(upp-low)
                rec = np.full(shape, low)
                
                weights.append(np.add(wght, rec))
                
        return weights
        
        
    def evaluate_particles(self):
        
        for particle in self.particles:
            self.model.set_weights(particle.get_weights())
            train_loss, train_acc = self.model.evaluate(self.training_x, self.training_y, verbose = 0)
            
            particle.update_valuation(train_acc)
            
            if(train_acc > self.best_evaluation):
                self.best_particle = particle.get_weights()
                self.best_evaluation = train_acc
        
        
    def isclose(self, a, b, rel_tol=1e-09, abs_tol=0.0):
        return abs(a-b) <= max(rel_tol * max(abs(a), abs(b)), abs_tol)
    
    def update_particles(self):
        
        for particle in self.particles:
            particle.update(self.best_particle)
            
    def get_particles(self):
        return self.particles
    
    def start_pso(self):
        
        for i in range(self.num_iters):
            partly_best_solution = self.best_evaluation
            self.update_particles()
            self.evaluate_particles()
            
            #print("Iteracija: {}\nPreciznost: {}\n".format(i+1, self.best_evaluation))
            
            if(self.isclose(partly_best_solution, self.best_evaluation, 0.001)):
                self.combo += 1
            else:
                self.combo = 0
                
            if(self.combo == 5):
                self.particles.sort(key = lambda x: x.best_valuation)
                
                velocity = self.make_velocity(self.shapes)
                
                weights = self.make_weights(self.shapes, self.lower_bound, self.upper_bound)
                self.particles[0] = Particle(weights, velocity, self.w, self.c1, self.c2, 
                                             self.lower_bound, self.upper_bound, self.particles[0].id)
                
                weights = self.make_weights(self.shapes, self.lower_bound, self.upper_bound)
                self.particles[1] = Particle(weights, velocity, self.w, self.c1, self.c2, 
                                             self.lower_bound, self.upper_bound, self.particles[1].id)
                
                weights = self.make_weights(self.shapes, self.lower_bound, self.upper_bound)
                self.particles[2] = Particle(weights, velocity, self.w, self.c1, self.c2, 
                                             self.lower_bound, self.upper_bound, self.particles[2].id)
                
                weights = self.make_weights(self.shapes, self.lower_bound, self.upper_bound)
                self.particles[3] = Particle(weights, velocity, self.w, self.c1, self.c2, 
                                             self.lower_bound, self.upper_bound, self.particles[3].id)
                
                weights = self.make_weights(self.shapes, self.lower_bound, self.upper_bound)
                self.particles[4] = Particle(weights, velocity, self.w, self.c1, self.c2, 
                                             self.lower_bound, self.upper_bound, self.particles[4].id)
                self.combo = 0
                self.evaluate_particles()
                #print("Change bad particles")
            
            self.history.append(self.best_evaluation)   
                
                
            
        return self.best_particle

\end{lstlisting}


\begin{lstlisting}
accs = []
print("IRIS DATASET: \n\n")
data1 = datasets.load_iris()

x_train, x_test, y_train, y_test = train_test_split(data1.data, 
                                                    data1.target, 
                                                    test_size=0.33)

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)


# Pravljenje neuronske mreze sa 3 sloja - ulazni, skriveni i izlazni
model = Sequential()
model.add(Dense(units=100, input_dim=x_train.shape[1], activation='relu'))
model.add(Dense(units=y_train.shape[1], activation='sigmoid'))
model.summary()

shapes = [i.shape for i in model.get_weights()]

model.compile(optimizer='adam', 
            loss=losses.categorical_crossentropy, 
            metrics=['accuracy'])

test_iris_acc = 0
train_iris_acc = 0
best_iris_pso = None

PSOS = []
for i in range(5):
    pso = PSO(30, 300, 
            x_train, y_train, 
            model, 
            shapes, 
            0.5, 1.0, 0.3, -2, 2)

    PSOS.append(pso)

for pso in PSOS:

    best_particle = pso.start_pso()

    model.set_weights(best_particle)

    train_loss, train_acc = model.evaluate(x_train, y_train)
    test_loss, test_acc = model.evaluate(x_test, y_test)

    if(test_iris_acc < test_acc): 
        test_iris_acc = test_acc
        best_iris_pso = pso
        train_iris_acc = train_acc
        
    print("Current PSO train accuracy: " + str(train_acc))
    print("Current PSO test accuracy:   " + str(test_acc) + "\n")

print()
print("Global best accuracy: " + str(test_iris_acc))
accs.append((train_iris_acc, test_iris_acc))

\end{lstlisting}


\begin{lstlisting}
print("\n---------------------------------\nBREAST CANCER DATASET: \n\n")
data2 = datasets.load_breast_cancer()

x_train, x_test, y_train, y_test = train_test_split(data2.data, 
                                                    data2.target, 
                                                    test_size=0.33)

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Pravljenje neuronske mreze sa 3 sloja - ulazni, skriveni i izlazni
model = Sequential()
model.add(Dense(units=100, input_dim=x_train.shape[1], activation='relu'))
model.add(Dense(units=y_train.shape[1], activation='sigmoid'))
model.summary()

shapes = [i.shape for i in model.get_weights()]

model.compile(optimizer='adam', 
            loss=losses.categorical_crossentropy, 
            metrics=['accuracy'])

test_cancer_acc = 0
train_cancer_acc = 0
best_cancer_pso = None

PSOS = []
for i in range(5):
    pso = PSO(30, 300, 
            x_train, y_train, 
            model, 
            shapes, 
            0.5, 1.0, 0.3, -2, 2)

    PSOS.append(pso)

for pso in PSOS:

    best_particle = pso.start_pso()

    model.set_weights(best_particle)

    train_loss, train_acc = model.evaluate(x_train, y_train)
    test_loss, test_acc = model.evaluate(x_test, y_test)

    if(test_cancer_acc < test_acc): 
        test_cancer_acc = test_acc
        train_cancer_acc = train_acc
        best_cancer_pso = pso

    print("Current PSO train accuracy: " + str(train_acc))
    print("Current PSO test accuracy:   " + str(test_acc) + "\n")

print()
print("Global best accuracy: " + str(test_cancer_acc))
accs.append((train_cancer_acc, test_cancer_acc))
\end{lstlisting}


\begin{lstlisting}
print("\n---------------------------------\nWINE DATASET: \n\n")
data3 = datasets.load_wine()

x_train, x_test, y_train, y_test = train_test_split(data3.data, 
                                                    data3.target, 
                                                    test_size=0.33)

y_train = to_categorical(y_train)
y_test = to_categorical(y_test)

# Pravljenje neuronske mreze sa 3 sloja - ulazni, skriveni i izlazni
model = Sequential()
model.add(Dense(units=100, input_dim=x_train.shape[1], activation='relu'))
model.add(Dense(units=y_train.shape[1], activation='sigmoid'))
model.summary()

shapes = [i.shape for i in model.get_weights()]

model.compile(optimizer='adam', 
            loss=losses.categorical_crossentropy, 
            metrics=['accuracy'])

test_wine_acc = 0
train_wine_acc = 0
best_wine_pso = None

PSOS = []
for i in range(5):
    pso = PSO(30, 300, 
            x_train, y_train, 
            model, 
            shapes, 
            0.5, 1.0, 0.3, -2, 2)

    PSOS.append(pso)

for pso in PSOS:

    best_particle = pso.start_pso()

    model.set_weights(best_particle)

    train_loss, train_acc = model.evaluate(x_train, y_train)
    test_loss, test_acc = model.evaluate(x_test, y_test)

    if(test_wine_acc < test_acc): 
        test_wine_acc = test_acc
        train_wine_acc = train_acc
        best_wine_pso = pso

    print("Current PSO train accuracy: " + str(train_acc))
    print("Current PSO test accuracy:   " + str(test_acc) + "\n")

print()
print("Global best accuracy: " + str(test_wine_acc))
accs.append((train_wine_acc, test_wine_acc))
\end{lstlisting}



\end{document}
